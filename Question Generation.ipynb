{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "_word_to_idx = {}\n",
    "_idx_to_word = []\n",
    "\n",
    "\n",
    "def _add_word(word):\n",
    "    idx = len(_idx_to_word)\n",
    "    _word_to_idx[word] = idx\n",
    "    _idx_to_word.append(word)\n",
    "    return idx\n",
    "\n",
    "PAD_WORD = \"<PAD>\"\n",
    "UNKNOWN_WORD = \"<UNK>\"\n",
    "START_WORD = \"<START>\"\n",
    "END_WORD = \"<END>\"\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "PAD_TOKEN = _add_word(PAD_WORD)\n",
    "UNKNOWN_TOKEN = _add_word(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word(START_WORD)\n",
    "END_TOKEN = _add_word(END_WORD)\n",
    "\n",
    "\n",
    "def look_up_word(word):\n",
    "    return _word_to_idx.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token(token):\n",
    "    return _idx_to_word[token]\n",
    "\n",
    "\n",
    "with open('/home/aditi/Desktop/6th sem/Computer Graphics/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    chunks = line.split(\" \")\n",
    "    dimensions = len(chunks) - 1\n",
    "    f.seek(0)\n",
    "\n",
    "    vocab_size = sum(1 for line in f)\n",
    "    vocab_size += 3\n",
    "    f.seek(0)\n",
    "\n",
    "    glove = np.ndarray((vocab_size, dimensions), dtype=np.float32)\n",
    "    glove[PAD_TOKEN] = np.zeros(dimensions)\n",
    "    glove[UNKNOWN_TOKEN] = np.zeros(dimensions)\n",
    "    glove[START_TOKEN] = -np.ones(dimensions)\n",
    "    glove[END_TOKEN] = np.ones(dimensions)\n",
    "\n",
    "    for line in f:\n",
    "        chunks = line.split(\" \")\n",
    "        idx = _add_word(chunks[0])\n",
    "        glove[idx] = [float(chunk) for chunk in chunks[1:]]\n",
    "        if len(_idx_to_word) >= vocab_size:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "# from embedding import *\n",
    "import nltk\n",
    "import itertools\n",
    "import random\n",
    "np.random.seed(0)\n",
    "orig_stdout = sys.stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in ./anaconda3/lib/python3.5/site-packages (2.0.18)\n",
      "Requirement already satisfied, skipping upgrade: regex==2018.01.10 in ./anaconda3/lib/python3.5/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in ./anaconda3/lib/python3.5/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in ./anaconda3/lib/python3.5/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in ./anaconda3/lib/python3.5/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in ./anaconda3/lib/python3.5/site-packages (from spacy) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: thinc<6.13.0,>=6.12.1 in ./anaconda3/lib/python3.5/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied, skipping upgrade: ujson>=1.35 in ./anaconda3/lib/python3.5/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in ./anaconda3/lib/python3.5/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in ./anaconda3/lib/python3.5/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.5/site-packages (from spacy) (2.14.2)\n",
      "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied, skipping upgrade: msgpack<0.6.0,>=0.5.6 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.6)\n",
      "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.0)\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.13.0,>=6.12.1->spacy)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.31.1)\n",
      "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in ./anaconda3/lib/python3.5/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.8.0)\n",
      "\u001b[31mthinc 6.12.1 has requirement dill<0.3.0,>=0.2.7, but you'll have dill 0.2.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: cytoolz\n",
      "  Found existing installation: cytoolz 0.8.0\n",
      "\u001b[31mCannot uninstall 'cytoolz'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0 in ./anaconda3/lib/python3.5/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/aditi/anaconda3/lib/python3.5/site-packages/en_core_web_lg -->\n",
      "    /home/aditi/anaconda3/lib/python3.5/site-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textacy in ./anaconda3/lib/python3.5/site-packages (0.6.2)\n",
      "Requirement already satisfied: python-levenshtein>=0.12.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: ijson>=2.3 in ./anaconda3/lib/python3.5/site-packages (from textacy) (2.3)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (3.1.0)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (0.8.0)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in ./anaconda3/lib/python3.5/site-packages (from textacy) (4.31.1)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.2.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (4.4.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (1.15.2)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in ./anaconda3/lib/python3.5/site-packages (from textacy) (0.9.5)\n",
      "Requirement already satisfied: unidecode>=0.04.19 in ./anaconda3/lib/python3.5/site-packages (from textacy) (1.0.23)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (0.20.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: spacy>=2.0.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (2.0.18)\n",
      "Requirement already satisfied: requests>=2.10.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (2.14.2)\n",
      "Requirement already satisfied: networkx>=1.11 in ./anaconda3/lib/python3.5/site-packages (from textacy) (1.11)\n",
      "Requirement already satisfied: pyemd>=0.3.0 in ./anaconda3/lib/python3.5/site-packages (from textacy) (0.5.1)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.5/site-packages (from python-levenshtein>=0.12.0->textacy) (40.0.0)\n",
      "Requirement already satisfied: wcwidth in ./anaconda3/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.1.7)\n",
      "Requirement already satisfied: html5lib in ./anaconda3/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.9999999)\n",
      "Requirement already satisfied: regex==2018.01.10 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (2018.1.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (2.0.2)\n",
      "Requirement already satisfied: ujson>=1.35 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (1.35)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (6.12.1)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (0.9.6)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in ./anaconda3/lib/python3.5/site-packages (from spacy>=2.0.0->textacy) (0.2.5)\n",
      "Requirement already satisfied: decorator>=3.4.0 in ./anaconda3/lib/python3.5/site-packages (from networkx>=1.11->textacy) (4.0.10)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.5/site-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (1.10.0)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (0.4.3.2)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (1.10.6)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in ./anaconda3/lib/python3.5/site-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (0.5.6)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('/home/aditi/Desktop/6th sem/Computer Graphics/train-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aditi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltkStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "#print(len(contexts))\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        if (qas[i][j][\"answers\"]):\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            start,end = find_sub_list(answer,passage)\n",
    "            if start == -1:\n",
    "                continue\n",
    "            a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsToTake = 80000\n",
    "words = findKMostFrequentWords(400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103291"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73218, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "#PAD_TOKEN = _add_word_reduced(PAD_WORD)\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "    if(len(reduced_glove) == wordsToTake):\n",
    "        break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aditi-Inspiron-13-5368\n",
      "44984\n",
      "foxxy cleopatra alongside mike myers in austin powers in goldmember , 73 million . work it out '' uk , norway , and belgium . musical comedy the fighting temptations as mixed reviews from missy elliott , summertime '' "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i], end = ' ', sep = ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beyoncé', 'giselle', 'knowles-carter', '(', '/biːˈjɒnseɪ/', 'bee-yon-say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl-group', 'destiny', \"'s\", 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'s\", 'best-selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyoncé', \"'s\", 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number-one', 'singles', '``', 'crazy', 'in', 'love', \"''\", 'and', '``', 'baby', 'boy', \"''\", '.']\n",
      "['in', 'the', 'late', '1990s']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 54)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44984\n",
      "['the', 'remaining', 'band', 'members', 'recorded', '``', 'independent', 'women', 'part', 'i', \"''\", ',', 'which', 'appeared', 'on', 'the', 'soundtrack', 'to', 'the', '2000', 'film', ',', 'charlie', \"'s\", 'angels', '.', 'it', 'became', 'their', 'best-charting', 'single', ',', 'topping', 'the', 'u.s.', 'billboard', 'hot', '100', 'chart', 'for', 'eleven', 'consecutive', 'weeks', '.', 'in', 'early', '2001', ',', 'while', 'destiny', \"'s\", 'child', 'was', 'completing', 'their', 'third', 'album', ',', 'beyoncé', 'landed', 'a', 'major', 'role', 'in', 'the', 'mtv', 'made-for-television', 'film', ',', 'carmen', ':', 'a', 'hip', 'hopera', ',', 'starring', 'alongside', 'american', 'actor', 'mekhi', 'phifer', '.', 'set', 'in', 'philadelphia', ',', 'the', 'film', 'is', 'a', 'modern', 'interpretation', 'of', 'the', '19th', 'century', 'opera', 'carmen', 'by', 'french', 'composer', 'georges', 'bizet', '.', 'when', 'the', 'third', 'album', 'survivor', 'was', 'released', 'in', 'may', '2001', ',', 'luckett', 'and', 'roberson', 'filed', 'a', 'lawsuit', 'claiming', 'that', 'the', 'songs', 'were', 'aimed', 'at', 'them', '.', 'the', 'album', 'debuted', 'at', 'number', 'one', 'on', 'the', 'u.s.', 'billboard', '200', ',', 'with', 'first-week', 'sales', 'of', '663,000', 'copies', 'sold', '.', 'the', 'album', 'spawned', 'other', 'number-one', 'hits', ',', '``', 'bootylicious', \"''\", 'and', 'the', 'title', 'track', ',', '``', 'survivor', \"''\", ',', 'the', 'latter', 'of', 'which', 'earned', 'the', 'group', 'a', 'grammy', 'award', 'for', 'best', 'r', '&', 'b', 'performance', 'by', 'a', 'duo', 'or', 'group', 'with', 'vocals', '.', 'after', 'releasing', 'their', 'holiday', 'album', '8', 'days', 'of', 'christmas', 'in', 'october', '2001', ',', 'the', 'group', 'announced', 'a', 'hiatus', 'to', 'further', 'pursue', 'solo', 'careers', '.']\n",
      "['mtv']\n",
      "['for', 'what', 'network', ',', 'did', 'beyonce', 'land', 'a', 'major', 'movie', 'role', 'in', '?']\n",
      "['san', 'diego', 'hosts', 'several', 'major', 'producers', 'of', 'wireless', 'cellular', 'technology', '.', 'qualcomm', 'was', 'founded', 'and', 'is', 'headquartered', 'in', 'san', 'diego', ',', 'and', 'is', 'one', 'of', 'the', 'largest', 'private-sector', 'employers', 'in', 'san', 'diego', '.', 'other', 'wireless', 'industry', 'manufacturers', 'headquartered', 'here', 'include', 'nokia', ',', 'lg', 'electronics', ',', 'kyocera', 'international.', ',', 'cricket', 'communications', 'and', 'novatel', 'wireless', '.', 'the', 'largest', 'software', 'company', 'in', 'san', 'diego', 'is', 'security', 'software', 'company', 'websense', 'inc.', 'san', 'diego', 'also', 'has', 'the', 'u.s.', 'headquarters', 'for', 'the', 'slovakian', 'security', 'company', 'eset', '.', 'san', 'diego', 'has', 'been', 'designated', 'as', 'an', 'ihub', 'innovation', 'center', 'for', 'collaboration', 'potentially', 'between', 'wireless', 'and', 'life', 'sciences', '.']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['security']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "['what', 'sector', 'does', 'eset', 'fall', 'under', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85335"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_comp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a tradeoff in examples to train because of memory constraints\n",
    "\n",
    "examples_to_take_train = 55000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766\n",
      "46\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1\n",
    "print(max_document_len)\n",
    "print(max_answer_len)\n",
    "print(max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 766)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73218, 100)\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(PAD_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "        if(j<max_len):\n",
    "            X_indices[i,j] = look_up_word_reduced(END_WORD)\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    8,    33,  3464,  1344,     4,  4643,     8,  1614,   602,\n",
       "           7,  7467,   130,  4194,    29,   480,    30,    60,  1157,\n",
       "           9,  2006,  2115,   585,     6,  4874,  2348,     4,  3098,\n",
       "           7,  6416,    13,   117,    13, 12995,  1645,  1688,     8,\n",
       "           3,  1670,   217,     4,   359,     3,  1754,     5,  4366,\n",
       "           4,    33,   215,  2851,     8,    65,   920,  5315,     5,\n",
       "           3,   155,    72,    84,     4,    85,  3162,    10,  1772,\n",
       "         336,     6,   101,     4,     3,    94,     5,     0,  3476,\n",
       "           4,    87,   903,   255,  1328,     4,  1638,    43,   294,\n",
       "        1390,     4,  2769,   146,    57,  1091,     9, 45276,     6,\n",
       "         102,  3002,   100,  3795,     4,     3,   130,    43,  7174,\n",
       "           3,   477,     8,  1942,     7, 10069,    42,   389,  1800,\n",
       "         480,   129,   294,  5954,     9,    10,  2772,   703,     6,\n",
       "         765,  7877,    29,   374, 37405,   653,     9,  5038,  2026,\n",
       "           7, 12679,     3,  1754,     5,  3160,     9,  1714,     3,\n",
       "          84,   597,  1919,     6,     2,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 766)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    start = 0\n",
    "    while start < len(inputs[0]):\n",
    "        end = min(len(inputs[0]), start + batch_size)\n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_mask': [],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[],\n",
    "                 }\n",
    "        \n",
    "        for index,inp in enumerate(inputs):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            \n",
    "            if index == 0:\n",
    "                output['document_tokens'].append(inp[start:end,0:maxD])\n",
    "            elif index==1:\n",
    "                output['document_lengths'].append(inp[start:end])\n",
    "            elif index==2:\n",
    "                output['answer_labels'].append(inp[start:end,0:maxD])\n",
    "            elif index==3:\n",
    "                output['answer_mask'].append(inp[start:end,0:maxA,0:maxD])\n",
    "            elif index==4:\n",
    "                output['answer_lengths'].append(inp[start:end])\n",
    "            elif index==5:\n",
    "                output['question_input_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==6:\n",
    "                output['question_output_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==7:\n",
    "                output['question_lengths'].append(inp[start:end])\n",
    "            \n",
    "        output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "        output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "        output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "        output[\"answer_mask\"] = np.array(output[\"answer_mask\"])\n",
    "        output[\"answer_lengths\"] = np.array(output[\"answer_lengths\"])\n",
    "        output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "        output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "        output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        outputs.append(output)\n",
    "        start = start + batch_size\n",
    "            \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths]\n",
    "                    ,batch_size)\n",
    "\n",
    "for b in batch_input:\n",
    "    for k, v in b.items():\n",
    "        b[k] = v.squeeze(0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
